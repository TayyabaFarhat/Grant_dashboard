# ğŸš€ LaunchPad Intelligence Dashboard

**A premium, auto-updating startup opportunities dashboard powered by GitHub Actions.**

Automatically discovers and displays startup competitions, grants, accelerators, fellowships, hackathons, and funding opportunities from across the internet â€” updated every 24 hours with zero manual intervention.

---

## âœ¨ Features

- **Auto-updated daily** via GitHub Actions (no manual work required)
- **Premium SaaS-quality UI** inspired by Stripe, Linear, and Notion
- **24+ pre-loaded opportunities** from top sources
- **Smart filtering** by type, country, status, and deadline
- **Live status badges** (Open, Closing Soon, New, Closed)
- **Dark mode** with elegant toggle
- **Fully responsive** â€” works on mobile, tablet, and desktop
- **Instant search** across all opportunity fields
- **Sources**: Devpost, Challenge.gov, F6S, Seedstars, EIC, Google News, Reddit, and more

---

## ğŸš€ Quick Deploy (5 minutes)

### Step 1: Create GitHub Repository

1. Go to [github.com/new](https://github.com/new)
2. Name it: `startup-intelligence-dashboard`
3. Set to **Public** (required for free GitHub Pages)
4. Click **Create repository**

### Step 2: Upload Files

**Option A â€” GitHub Web UI (easiest):**
1. Open your new repository
2. Click **Add file â†’ Upload files**
3. Drag and drop ALL project files/folders
4. Maintain the folder structure exactly
5. Click **Commit changes**

**Option B â€” Git CLI:**
```bash
git clone https://github.com/YOUR_USERNAME/startup-intelligence-dashboard.git
cd startup-intelligence-dashboard
# Copy all project files here
git add .
git commit -m "Initial deploy"
git push
```

### Step 3: Enable GitHub Pages

1. In your repository, go to **Settings â†’ Pages**
2. Under **Source**, select **Deploy from a branch**
3. Select branch: `main` (or `master`)
4. Select folder: `/ (root)`
5. Click **Save**

â³ Wait 2-3 minutes, then visit:
```
https://YOUR_USERNAME.github.io/startup-intelligence-dashboard/
```

### Step 4: Verify Automation

1. Go to **Actions** tab in your repository
2. You'll see the `Auto-Update Opportunities` workflow
3. Click **Run workflow** to test it immediately
4. It will automatically run every 24 hours from now on

---

## ğŸ“ Project Structure

```
startup-intelligence-dashboard/
â”‚
â”œâ”€â”€ index.html              # Main dashboard page
â”œâ”€â”€ style.css               # Premium SaaS styling
â”œâ”€â”€ script.js               # Dashboard logic & filtering
â”œâ”€â”€ opportunities.json      # Data file (auto-updated)
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ scraper.py          # Main scraper (all sources)
â”‚   â”œâ”€â”€ social_scraper.py   # Social media scraper
â”‚   â””â”€â”€ requirements.txt    # Python dependencies
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ auto-update.yml     # GitHub Actions automation
â”‚
â””â”€â”€ README.md               # This file
```

---

## ğŸ¤– How Automation Works

```
Every 24 hours (midnight UTC):
â”‚
â”œâ”€â”€ GitHub Actions triggers automatically
â”œâ”€â”€ Installs Python + dependencies
â”œâ”€â”€ Runs scraper.py
â”‚   â”œâ”€â”€ Scrapes Devpost RSS
â”‚   â”œâ”€â”€ Scrapes Challenge.gov API
â”‚   â”œâ”€â”€ Scrapes Google News RSS
â”‚   â”œâ”€â”€ Scrapes EIC (EU grants)
â”‚   â”œâ”€â”€ Scrapes Seedstars
â”‚   â””â”€â”€ Scrapes F6S
â”œâ”€â”€ Merges with existing data
â”œâ”€â”€ Removes duplicates
â”œâ”€â”€ Saves to opportunities.json
â”œâ”€â”€ Commits & pushes to GitHub
â””â”€â”€ GitHub Pages auto-deploys âœ…
```

No servers. No paid APIs. No maintenance required.

---

## ğŸ“Š Data Sources

| Source | Type | Method |
|--------|------|--------|
| Devpost | Hackathons | RSS Feed |
| Challenge.gov | US Gov Grants | Public API |
| Google News | All types | RSS Feed |
| EU EIC | EU Grants | Web scraping |
| Seedstars | Competitions | Web scraping |
| F6S | Accelerators | Web scraping |
| Reddit | Community | RSS Feed |
| LinkedIn | Social signals | Via Google News |

---

## â• Adding New Sources

Edit `scraper/scraper.py` and add a new function:

```python
def scrape_my_source():
    log.info("Scraping My Source...")
    opps = []
    
    resp = safe_get("https://example.com/opportunities.rss")
    if not resp:
        return opps
    
    soup = BeautifulSoup(resp.text, "xml")
    for item in soup.find_all("item")[:20]:
        opps.append(normalize_opp({
            "name": item.find("title").get_text(strip=True),
            "organization": "My Source",
            "category": "Competition",
            "type": "competition",  # grant | competition | accelerator | hackathon | fellowship | funding
            "country": "Global",
            "link": item.find("link").get_text(strip=True),
            "source": "example.com",
            "description": "...",
            "tags": ["my-tag"],
        }))
    
    return opps
```

Then add it to the `scrapers` list in `main()`.

---

## ğŸ¨ Customization

### Change Colors
Edit CSS variables in `style.css`:
```css
:root {
  --accent: #6d56fa;     /* Primary accent color */
  --bg: #f8f8f6;         /* Background color */
  /* ... */
}
```

### Add Categories
In `index.html`, add a new tab:
```html
<button class="tab" data-tab="mytype">My Category</button>
```

In `style.css`, add type badge style:
```css
.type-mytype { background: #e0f2fe; color: #0284c7; }
```

### Manual Data Entry
Add entries directly to `opportunities.json`:
```json
{
  "id": "custom001",
  "name": "My Opportunity",
  "organization": "My Org",
  "category": "Grant",
  "type": "grant",
  "country": "United States",
  "deadline": "2026-12-31",
  "prize": "$50,000",
  "link": "https://example.com",
  "source": "manual",
  "date_added": "2026-02-28T00:00:00Z",
  "status": "open",
  "description": "Description here.",
  "tags": ["tag1", "tag2"]
}
```

---

## ğŸ›  Local Development

```bash
# Install Python dependencies
pip install -r scraper/requirements.txt

# Run scraper locally
python scraper/scraper.py

# Serve locally (Python built-in server)
python -m http.server 8000
# â†’ Visit http://localhost:8000
```

---

## ğŸ“„ License

MIT License â€” free for personal and commercial use.

---

## ğŸ™ Data Sources Acknowledgment

Data is collected from publicly available sources including Devpost, Challenge.gov (US Government public data), Google News RSS, EU Innovation Council, Seedstars, F6S, and Reddit. All links direct to original sources. This dashboard is an aggregator for informational purposes.
